Predicting Health Misinformation: Myth/Fact Classification Report

1. Introduction
The spread of health misinformation (myths) on digital platforms poses a serious threat to public health awareness and decision-making. This case study presents a machine learning-based approach to classify health-related statements as either a Fact (0) or a Myth (1).

By combining the Random Forest Classifier with TF-IDF (Term Frequency–Inverse Document Frequency) text vectorization, the model ensures accurate classification and interpretability.
The objective is to develop a reliable, automated system that promotes evidence-based health information and counters misinformation effectively.

2. Dataset Overview
The dataset was specifically designed for binary text classification of health-related claims. It integrates both web-scraped data and synthetic data to achieve better coverage and balance.

Key Attributes:

Text/Statement: The health claim or statement (feature).

Label: Target variable — Myth (1) or Fact (0).

Data Sources:
Data was collected through web scraping from multiple trusted medical sources such as:

WebMD

Mayo Clinic

Healthline

GenPsych

UCHealth

To enhance generalization, a synthetic dataset was also generated by augmenting scraped data with paraphrased health statements.

3. Data Preprocessing:

Text Vectorization: Raw statements were converted to numerical form using TF-IDF, assigning higher weights to rare yet informative words.

Class Balancing: A SMOTE-like augmentation technique was applied to ensure a balanced distribution of Fact and Myth samples, minimizing class bias.

Methodology / Approach
The project followed a structured machine learning pipeline focused on text-based misinformation detection.

a) Feature Engineering (TF-IDF)
The primary preprocessing step used TF-IDF vectorization to represent each statement numerically.
This method captures the importance of specific terms relative to the entire dataset, allowing the model to distinguish meaningful words like “causes,” “prevents,” “risk,” etc.

b) Handling Imbalance with SMOTE
To address class imbalance between Myth and Fact statements, a SMOTE-like oversampling technique was applied.
This ensured equal representation of both classes during model training, promoting fairness and improving recall.

c) Model Building: Random Forest Classifier
The Random Forest Classifier was chosen for its:

Robustness in handling large, high-dimensional text data.

Ability to capture non-linear feature relationships.

Resistance to overfitting and interpretability through feature importance.

d) Evaluation Metrics
Model performance was evaluated using:

Accuracy

Confusion Matrix

Precision, Recall, and F1-Score

Cross-Validation to ensure stability across different data splits.

4. Results and Discussion
The trained Random Forest model demonstrated strong performance with a Test Accuracy of 88.7% (~89%), aligning with the target goal.

Confusion Matrix:

	Predicted Fact (0)	Predicted Myth (1)
Actual Fact (0)	133 (True Negatives)	17 (False Positives)
Actual Myth (1)	17 (False Negatives)	133 (True Positives)

Performance Interpretation:

High counts of True Positives and True Negatives (133 each) indicate robust classification.

Low False Positives (17) and False Negatives (17) confirm minimal misclassification.

The model effectively identifies myths without over-flagging true facts.

5. Visualization Gallery
To better understand model behavior, multiple visualizations were created:

Feature Importance Plot: Showed key terms and n-grams influencing model decisions.

Confusion Matrix Heatmap: Illustrated prediction distribution and overall accuracy.

SMOTE Distribution Plot: Validated dataset balancing before training.

Cross-Validation Plot: Confirmed model stability and consistent accuracy across folds.

6. Prediction Example
A simple test example demonstrates the model’s interpretability and practical use.

Example Input:

“A daily vitamin C supplement can prevent the flu.”

Predicted Output:

Myth (1)

Prediction Result:

The model classifies this statement as a Myth and provides a factual explanation debunking the claim based on medical data.

7. Conclusion
This study successfully developed an explainable and reliable Random Forest-based health misinformation detection model using TF-IDF.
The achieved accuracy of 89% proves the model’s capability to detect myths effectively and support public health awareness.

8. Future Enhancements:

Advanced Models: Experimenting with deep learning architectures like BERT or CNNs for contextual understanding.

Feature Expansion: Incorporating metadata such as source credibility, date, or author.

Dataset Diversification: Extending the dataset to include varied misinformation topics and multilingual data.

9. References / Tools Used

Tools & Libraries:

Programming Language: Python 3.x

ML Libraries: pandas, numpy, scikit-learn, imbalanced-learn, matplotlib, seaborn

Algorithm: Random Forest Classifier

Vectorization Technique: TF-IDF

Data Sources:

Web-scraped data from WebMD, Mayo Clinic, Healthline, GenPsych, and UCHealth

Synthetic dataset generated through paraphrasing and augmentation
