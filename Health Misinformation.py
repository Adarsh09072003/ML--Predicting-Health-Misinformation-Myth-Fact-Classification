# -*- coding: utf-8 -*-
"""ML Final(M).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J9_NJSDBTOWxJ4qyrs99rLhqaRPqtR-D
"""

pip install scikit-learn pandas numpy beautifulsoup4 requests

import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
from typing import List, Dict
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, f1_score, recall_score
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# Step 1: Enhanced Web Scraping Function (Updated for 2025 Sources)
def scrape_health_myths(url: str, source_name: str = "WebMD") -> List[Dict[str, str]]:
    """
    Scrapes myths and facts from a webpage. Supports multiple sources.
    Returns list of dicts: [{'text': 'myth text', 'label': 1, 'fact': 'corresponding fact', 'source': 'source_name'}]
    """
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        myths = []
        if source_name == "WebMD":
            # WebMD slideshow structure (e.g., https://www.webmd.com/balance/ss/slideshow-10-health-myths-debunked)
            slides = soup.find_all('div', class_='slide') or soup.find_all('li', class_='slide')
            for slide in slides:
                myth_elem = slide.find('h3') or slide.find('h2') or slide.find('strong')
                fact_elem = slide.find('p') or slide.find('div', class_='fact')
                if myth_elem:
                    myth = re.sub(r'\*Myth:\*?', '', myth_elem.text.strip()).strip()
                    if fact_elem:
                        fact = re.sub(r'\*Fact:\*?', '', fact_elem.text.strip()).strip()
                    else:
                        fact = "Debunked: No evidence supports this claim."
                    if myth and len(myth) > 10:
                        myths.append({'text': myth, 'label': 1, 'fact': fact, 'source': source_name})
        elif source_name == "MayoClinic":
            # Mayo Clinic list structure (e.g., https://www.mayoclinic.org/healthy-lifestyle/adult-health/in-depth/health-myths/art-20046490)
            articles = soup.find_all('article') or soup.find_all('div', class_='article-body')
            for art in articles:
                myth = art.find('h3') or art.find('h2')
                fact = art.find('p')
                if myth and fact:
                    myth_text = re.sub(r'Myth:?', '', myth.text.strip()).strip()
                    fact_text = re.sub(r'Fact:?', '', fact.text.strip()).strip()
                    myths.append({'text': myth_text, 'label': 1, 'fact': fact_text, 'source': source_name})
        elif source_name == "Healthline":
            # Healthline (e.g., https://www.healthline.com/health-news/medical-myths-all-in-our-heads)
            items = soup.find_all('li') or soup.find_all('div', class_='myth-item')
            for item in items:
                text = item.text.strip()
                if 'myth' in text.lower() and len(text) > 20:
                    # Simple split for myth/fact
                    parts = text.split('However,')
                    if len(parts) > 1:
                        myth = parts[0].strip()
                        fact = 'However, ' + parts[1].strip()
                        myths.append({'text': myth, 'label': 1, 'fact': fact, 'source': source_name})
        else:
            # Generic fallback: Look for <h3> or <h2> as myths, next <p> as facts
            headers = soup.find_all(['h1', 'h2', 'h3'])
            paragraphs = soup.find_all('p')
            for i, h in enumerate(headers):
                myth = h.text.strip()
                if i < len(paragraphs):
                    fact = paragraphs[i].text.strip()
                    if len(myth) > 10 and len(fact) > 10:
                        myths.append({'text': myth, 'label': 1, 'fact': fact, 'source': source_name})

        return myths
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return []

# Step 2: Scrape Multiple Sources (2025 Updated URLs)
urls = [
    {"url": "https://www.webmd.com/balance/ss/slideshow-10-health-myths-debunked", "source": "WebMD"},
    {"url": "https://www.mayoclinic.org/healthy-lifestyle/adult-health/in-depth/health-myths/art-20046490", "source": "MayoClinic"},
    {"url": "https://www.healthline.com/health-news/medical-myths-all-in-our-heads", "source": "Healthline"},
    {"url": "https://www.houstonmethodist.org/blog/articles/2025/jan/5-common-weight-loss-myths-debunked/", "source": "HoustonMethodist"},  # 2025
    {"url": "https://www.genpsych.com/blog/mental-health-myths/", "source": "GenPsych"},  # Updated
    {"url": "https://www.uchealth.org/today/10-health-myths-debunked/", "source": "UCHealth"},
    # Add more 2025 sources as needed
]

scraped_data = []
for config in urls:
    myths = scrape_health_myths(config["url"], config["source"])
    scraped_data.extend(myths)
    print(f"Scraped {len(myths)} entries from {config['source']}")

# Step 3: Generate Facts from Scraped Myths (for balance) and Augment
def generate_facts_from_myths(myths: List[Dict]) -> List[Dict]:
    facts = []
    for myth in myths:
        fact_entry = myth.copy()
        fact_entry['text'] = re.sub(r"Myth: ?", "Fact: ", myth['text']).replace("This is false.", "This is true.")
        fact_entry['label'] = 0
        facts.append(fact_entry)
    return facts

def augment_dataset(data: List[Dict], num_variations: int = 3) -> List[Dict]:
    """Generate variations by simple paraphrasing."""
    variations = []
    prefixes = ["People believe: ", "Common myth: ", "Many think: ", "Some say: "]
    for entry in data:
        variations.append(entry)  # Original
        for i in range(num_variations):
            var = entry.copy()
            prefix = prefixes[i % len(prefixes)] if entry['label'] == 1 else ""
            var['text'] = prefix + entry['text']
            variations.append(var)
    return variations

# Combine and Augment
all_myths = scraped_data
all_facts = generate_facts_from_myths(all_myths)
full_data = all_myths + all_facts
augmented_data = augment_dataset(full_data, num_variations=3)

df = pd.DataFrame(augmented_data)
print(f"Total dataset size: {len(df)}")
print(df['label'].value_counts())

# Step 4: Save to CSV (Downloadable)
csv_filename = 'web_scraped_health_myths_2025.csv'
df.to_csv(csv_filename, index=False)
print(f"Dataset saved to {csv_filename}. You can now download it!")

# Step 5: Quick Model Training (To Verify ~85% Accuracy)
X = df['text']
y = df['label']
vectorizer = TfidfVectorizer(max_features=1500, stop_words='english', ngram_range=(1,2))
X_vec = vectorizer.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42, stratify=y)
clf = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, class_weight='balanced')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Model Performance: Accuracy={acc:.3f}, F1={f1:.3f}, Recall={recall:.3f}")

# Step 6: Prediction Function (As Before)
def predict_and_debunk(input_text: str, clf_model, vectorizer, df):
    input_vec = vectorizer.transform([input_text])
    is_myth = clf_model.predict(input_vec)[0]
    confidence = clf_model.predict_proba(input_vec)[0].max()
    if is_myth == 1:
        sim_scores = cosine_similarity(input_vec, vectorizer.transform(df['text']))[0]
        best_match_idx = np.argmax(sim_scores)
        fact = df.iloc[best_match_idx]['fact']
        return {'prediction': 'Myth', 'confidence': confidence, 'fact': fact}
    else:
        return {'prediction': 'Fact', 'confidence': confidence, 'fact': 'No debunk needed.'}

# Example
test_input = "Does cracking your knuckles cause arthritis?"
result = predict_and_debunk(test_input, clf, vectorizer, df)
print(result)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, recall_score
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Full Base Data (Expanded to ~30 Myths + ~30 Facts, including 2025 updates from sources like EUFIC, Houston Methodist, GenPsych, UCHealth)
base_myths = [
    {'text': 'You must give up favorite foods to lose weight.', 'label': 1, 'fact': "You don't have to eliminate foods; focus on portion control and balance."},
    {'text': 'Grains are fattening.', 'label': 1, 'fact': 'Whole grains provide fiber and nutrients; choose refined ones sparingly.'},
    {'text': 'Gluten-free is always healthier.', 'label': 1, 'fact': 'Only for celiac; otherwise, whole grains with gluten lower disease risk.'},
    {'text': 'More men die from prostate cancer than any other.', 'label': 1, 'fact': 'Lung cancer leads; prostate is second but preventable with screenings.'},
    {'text': 'Men do not get depression.', 'label': 1, 'fact': 'Men experience depression but may show it as anger; seek help early.'},
    {'text': 'Eating healthy is too expensive.', 'label': 1, 'fact': 'Beans, eggs, and seasonal produce are affordable staples.'},
    {'text': 'Unrefined sugars like honey are better.', 'label': 1, 'fact': 'All sugars impact blood sugar similarly; moderation matters.'},
    {'text': 'Only certain people suffer from mental health issues.', 'label': 1, 'fact': '1 in 5 adults face them annually; anyone can.'},
    {'text': 'Mental health issues are rare.', 'label': 1, 'fact': 'They affect millions; stigma prevents seeking care.'},
    {'text': 'Anxiety/depression are just phases.', 'label': 1, 'fact': 'They can be chronic; therapy and meds help long-term.'},
    {'text': 'Do eggs cause high cholesterol?', 'label': 1, 'fact': 'Dietary cholesterol has little impact; focus on saturated fats.'},
    {'text': '10,000 steps is the key to health.', 'label': 1, 'fact': 'Any walking helps; aim for 150 min/week activity.'},
    {'text': 'You need 8 glasses of water daily.', 'label': 1, 'fact': 'Needs vary; thirst and urine color guide intake.'},
    {'text': 'Eggs are bad for your heart.', 'label': 1, 'fact': '1-2/day safe for most; nutrients outweigh risks.'},
    {'text': 'Antiperspirant causes cancer.', 'label': 1, 'fact': 'No evidence; aluminum absorption is minimal.'},
    {'text': 'Being cold gives you a cold.', 'label': 1, 'fact': 'Viruses cause colds; cold weather spreads them indoors.'},
    {'text': 'Ginger ale cures stomachaches.', 'label': 1, 'fact': 'Sugar water; ginger tea may soothe nausea.'},
    {'text': 'Cracking knuckles causes arthritis.', 'label': 1, 'fact': 'Harmless; no joint damage link.'},
    {'text': 'Sugar causes hyperactivity in kids.', 'label': 1, 'fact': 'Expectation effect; no direct cause.'},
    {'text': 'Vaccines cause autism.', 'label': 1, 'fact': 'No link; large studies confirm safety.'},
    {'text': 'Chocolate causes acne.', 'label': 1, 'fact': 'No causal link; hormones/genetics primary.'},
    # 2025 Additions from EUFIC Nutrition Myths
    {'text': 'Carbs make you gain weight.', 'label': 1, 'fact': "Weight gain occurs when calorie intake exceeds expenditure, regardless of the source. High-fibre carbs like whole grains, fruits, and vegetables promote fullness and reduce chronic disease risk."},
    {'text': 'All ultra-processed foods are bad for you.', 'label': 1, 'fact': "Processing varies; some add excessive fat, salt, or sugar, while others like whole grain breads and fortified plant-based alternatives are nutritious. Health depends on specific nutrient content."},
    {'text': 'Seed oils cause a rise in chronic disease.', 'label': 1, 'fact': "Seed oils high in omega-6 fatty acids do not increase inflammation; higher linoleic acid levels are linked to lower inflammation. Chronic diseases are more associated with saturated fats, salt, sugar, and sedentary lifestyles."},
    {'text': 'Fresh fruit & veg is better than frozen.', 'label': 1, 'fact': "Frozen, canned, and dried fruits and vegetables retain nutrients when processed at peak ripeness. All forms count toward the 5-a-day goal and are nutritious choices."},
    {'text': 'Coconut oil is a healthy choice.', 'label': 1, 'fact': "Coconut oil is 92% saturated fat, raising LDL cholesterol and heart disease risk. It does not improve cholesterol or body composition compared to other fats."},
    {'text': 'Low fat or fat free foods are always healthier.', 'label': 1, 'fact': "These labels only indicate low fat content but do not account for added sugars or salt, which can increase chronic disease risk."},
    {'text': 'Intermittent fasting is the best way to lose weight.', 'label': 1, 'fact': "Weight loss from intermittent fasting is due to calorie deficit, not fasting itself. It is not superior to regular calorie-restricted diets and may cause side effects."},
    {'text': 'You need to eat meat to get enough protein.', 'label': 1, 'fact': "Plant-based foods like beans, lentils, nuts, and whole grains provide sufficient protein. Combining them ensures a complete amino acid profile."},
    # From Houston Methodist Weight Myths
    {'text': 'Obesity is primarily caused by a lack of willpower and self-control.', 'label': 1, 'fact': "Obesity is a complex chronic disease influenced by genetics, sleep, stress, access to healthy food, and policy decisions."},
    {'text': 'People with obesity are less active.', 'label': 1, 'fact': "Most Americans do not meet physical activity guidelines; environment plays a larger role than weight."},
    # From GenPsych Mental Health
    {'text': 'Treatment Is Only Necessary for Severe Cases of Mental Illness.', 'label': 1, 'fact': "Mental health care benefits mild to moderate cases too, preventing worsening; it's as important as physical health care."},
    {'text': 'Once You’re Diagnosed, You Can Never Recover.', 'label': 1, 'fact': "A diagnosis is the start of healing; many achieve stability with proper care."},
    # From UCHealth
    {'text': 'Breakfast is the most important meal of the day.', 'label': 1, 'fact': "All three meals are equally important; skipping breakfast is common but not ideal."},
    {'text': 'Cough vigorously to save yourself during a heart attack.', 'label': 1, 'fact': "Call 911 immediately; coughing does not help."},
    {'text': 'Laughter is the best medicine.', 'label': 1, 'fact': "It reduces stress, but regular checkups are more impactful for health."},
    {'text': 'I need to get the strongest SPF sunblock possible to be protected.', 'label': 1, 'fact': "SPF 30 blocks 97% UV; higher offers minimal extra benefit, and all are safe."},
    {'text': 'My child’s fever is caused by teething.', 'label': 1, 'fact': "Fevers signal infection; teething and colds coincide but are unrelated."},
]

base_facts = [
    {'text': 'Balanced diet includes favorite foods in moderation.', 'label': 0, 'fact': 'Sustainable weight loss emphasizes variety over restriction.'},
    {'text': 'Whole grains reduce heart disease risk.', 'label': 0, 'fact': 'High fiber aids digestion and fullness.'},
    {'text': 'Gluten-containing whole grains benefit most people.', 'label': 0, 'fact': 'They provide B vitamins and fiber.'},
    {'text': 'Lung cancer is the leading cause of death in men.', 'label': 0, 'fact': 'Quit smoking and screen early.'},
    {'text': 'Men benefit from mental health screenings.', 'label': 0, 'fact': 'Early intervention improves outcomes.'},
    {'text': 'Affordable healthy eating starts with planning.', 'label': 0, 'fact': 'Beans, eggs, and seasonal produce are affordable staples.'},
    {'text': 'All sugars should be limited equally.', 'label': 0, 'fact': 'Added sugars raise risks; natural in fruit is fine.'},
    {'text': 'Mental health affects all demographics.', 'label': 0, 'fact': 'Support networks and access reduce stigma.'},
    {'text': 'Mental issues require professional treatment.', 'label': 0, 'fact': 'Therapy/CBT effective for lasting relief.'},
    {'text': 'Walking any amount improves health.', 'label': 0, 'fact': 'Consistency over exact steps.'},
    {'text': 'Eggs provide high-quality protein.', 'label': 0, 'fact': 'Nutrient-dense for muscle and eye health.'},
    {'text': 'Hydration comes from all fluids.', 'label': 0, 'fact': 'Tea, fruits count toward needs.'},
    {'text': 'Deodorants are safe for daily use.', 'label': 0, 'fact': 'Choose based on skin type.'},
    {'text': 'Colds spread via contact, not temperature.', 'label': 0, 'fact': 'Wash hands to prevent.'},
    {'text': 'Carrots support eye health with vitamin A.', 'label': 0, 'fact': "They maintain vision but don't improve it."},
    {'text': 'Regular exercise boosts immunity.', 'label': 0, 'fact': '150 min/week moderate activity recommended.'},
    {'text': 'Prostate screenings save lives.', 'label': 0, 'fact': 'Start at age 50 or earlier if risky.'},
    {'text': 'Social connections enhance longevity.', 'label': 0, 'fact': 'Strong ties reduce mortality risk.'},
    # 2025 Additions: Rephrased from myths as facts
    {'text': 'High-fibre carbs promote fullness and reduce disease risk.', 'label': 0, 'fact': "Weight gain is from calorie surplus; choose whole grains, fruits, veggies."},
    {'text': 'Nutritious ultra-processed foods exist like fortified alternatives.', 'label': 0, 'fact': "Evaluate based on nutrients, not just processing level."},
    {'text': 'Seed oils do not increase inflammation.', 'label': 0, 'fact': "Omega-6 linked to lower inflammation; focus on overall diet."},
    {'text': 'Frozen fruits and veggies retain nutrients.', 'label': 0, 'fact': "Processed at peak ripeness; all forms count toward daily goals."},
    {'text': 'Choose unsaturated fats over coconut oil.', 'label': 0, 'fact': "Saturated fats raise LDL; opt for olive or avocado oil."},
    {'text': 'Low-fat foods may have added sugars; check labels.', 'label': 0, 'fact': "Health depends on overall ingredients."},
    {'text': 'Calorie deficit drives weight loss, not fasting method.', 'label': 0, 'fact': "Any sustainable approach works; monitor energy levels."},
    {'text': 'Plant proteins from beans and grains suffice.', 'label': 0, 'fact': "Combine for complete amino acids; no meat needed."},
    {'text': 'Whole grains with gluten lower chronic disease risk.', 'label': 0, 'fact': "Avoid gluten-free unless diagnosed; they may add sugars."},
    {'text': 'Obesity influenced by genetics and environment.', 'label': 0, 'fact': "Not just willpower; access to food and activity matters."},
    {'text': 'Physical activity benefits everyone regardless of weight.', 'label': 0, 'fact': "Environment enables more movement than genetics."},
    {'text': 'Mental health treatment prevents escalation.', 'label': 0, 'fact': "Seek early for mild issues; comparable to physical care."},
    {'text': 'Recovery from mental diagnosis is possible.', 'label': 0, 'fact': "With care, stability and happiness achievable."},
    {'text': 'All meals provide essential nutrition.', 'label': 0, 'fact': "Don't skip any; balance throughout the day."},
    {'text': 'Call emergency services for heart attack symptoms.', 'label': 0, 'fact': "Immediate action saves lives."},
    {'text': 'Regular checkups are key to well-being.', 'label': 0, 'fact': "Laughter helps stress, but prevention is primary."},
    {'text': 'SPF 30 offers sufficient UV protection.', 'label': 0, 'fact': "Higher SPFs provide marginal gains; use generously."},
    {'text': 'Fevers indicate potential infection in children.', 'label': 0, 'fact': "Monitor closely; unrelated to teething."},
]

# Augmentation (21 variations for ~1,260 myths + ~630 facts = ~1,890 rows total)
def augment_entry(entry, num_variations=21):
    prefixes = [
        'Studies show ', 'Research confirms ', 'It is well-known that ', 'The fact is ', 'Science proves ', 'Experts agree ',
        'Evidence indicates ', 'It is known that ', 'Medical research shows ', 'Health experts say ', 'Scientific studies confirm ',
        'According to experts ', 'Research indicates ', 'Studies reveal ', 'Facts show ', 'It is established that ',
        'Evidence supports ', 'Science tells us ', 'Health science confirms ', 'Studies demonstrate ', 'Research proves '
    ]
    variations = []
    for prefix in prefixes:
        var = entry.copy()
        var['text'] = prefix + entry['text'].lower()
        variations.append(var)
    return variations

def generate_full_dataset():
    all_myths = [item for myth in base_myths for item in augment_entry(myth)]
    all_facts = [item for fact in base_facts for item in augment_entry(fact)]
    dataset = all_myths + all_facts
    np.random.seed(42)
    dataset = np.random.permutation(dataset).tolist()
    df = pd.DataFrame(dataset)
    df['label'] = df['label'].astype(int)
    return df

# Generate and Save Dataset
df = generate_full_dataset()
df.to_csv('health_myths_facts_2025.csv', index=False)
print(f"Dataset generated and saved: {len(df)} samples")
print(df['label'].value_counts())

# Train Model
def train_model(df):
    X = df['text']
    y = df['label']
    vectorizer = TfidfVectorizer(max_features=1500, stop_words='english', ngram_range=(1,2), min_df=3)
    X_vec = vectorizer.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.3, random_state=42, stratify=y)
    clf = RandomForestClassifier(n_estimators=150, max_depth=12, min_samples_split=8, min_samples_leaf=4, random_state=42, class_weight='balanced')
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    plt.show()
    return clf, vectorizer, df, acc, cv_acc, f1, recall

clf, vectorizer, df_data, acc, cv_acc, f1, recall = train_model(df)

# Prediction Function
def predict_and_debunk(input_text, clf, vectorizer, df):
    input_vec = vectorizer.transform([input_text])
    is_myth = clf.predict(input_vec)[0]
    prob = clf.predict_proba(input_vec)[0].max()
    if is_myth == 1:
        sim_scores = cosine_similarity(input_vec, vectorizer.transform(df['text']))[0]
        best_idx = np.argmax(sim_scores)
        fact = df.iloc[best_idx]['fact']
        return f"Myth - Conf: {prob:.2f}", fact
    else:
        return f"Fact - Conf: {prob:.2f}", "Evidence-based health knowledge."

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from typing import Dict

def predict_and_debunk_interface(input_text: str, clf_model, vectorizer, df) -> Dict:
    """
    Predicts if the input text is a myth or fact and provides a debunking fact if it's a myth.
    """
    input_vec = vectorizer.transform([input_text])
    is_myth = clf_model.predict(input_vec)[0]
    prob = clf_model.predict_proba(input_vec)[0].max() # Get confidence score

    if is_myth == 1:
        # Find most similar myth in dataset for fact retrieval
        # Use the vectorizer fitted on the full dataset
        sim_scores = cosine_similarity(input_vec, vectorizer.transform(df['text']))[0]
        best_match_idx = np.argmax(sim_scores)
        fact = df.iloc[best_match_idx]['fact']
        return {'prediction': 'Myth', 'confidence': prob, 'fact': fact}
    else:
        return {'prediction': 'Fact', 'confidence': prob, 'fact': 'Evidence-based health knowledge.'}

# Interactive input loop
print("--- Health Myth/Fact Classifier Interface ---")
print("Enter a health statement below.")
print("Type 'quit' to exit the interface.")
print("---------------------------------------------")

while True:
    user_input = input("Your statement: ")
    if user_input.lower() == 'quit':
        print("Exiting interface. Goodbye!")
        break
    if user_input.strip() == "":
        print("Please enter a statement.")
        continue

    result = predict_and_debunk_interface(user_input, clf, vectorizer, df)
    print(f"Prediction: {result['prediction']} (Confidence: {result['confidence']:.2f})")
    print(f"Fact: {result['fact']}\n")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Assuming 'y_test' and 'y_pred' are available from the model training in previous cells
# If not, you would need to train the model and get predictions first

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix
fig, ax = plt.subplots(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, xticklabels=['Fact (0)', 'Myth (1)'], yticklabels=['Fact (0)', 'Myth (1)'])
ax.set_title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming 'y_train' and 'y_train_smote' are available from the previous SMOTE cell

# Get class counts before SMOTE
original_counts = pd.Series(y_train).value_counts()

# Get class counts after SMOTE
smote_counts = pd.Series(y_train_smote).value_counts()

# Create subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Plot original class distribution
sns.barplot(x=original_counts.index, y=original_counts.values, ax=axes[0])
axes[0].set_title('Class Distribution Before SMOTE (Training Data)')
axes[0].set_xlabel('Class')
axes[0].set_ylabel('Count')
axes[0].set_xticks([0, 1])
axes[0].set_xticklabels(['Fact (0)', 'Myth (1)'])


# Plot class distribution after SMOTE
sns.barplot(x=smote_counts.index, y=smote_counts.values, ax=axes[1])
axes[1].set_title('Class Distribution After SMOTE (Training Data)')
axes[1].set_xlabel('Class')
axes[1].set_ylabel('Count')
axes[1].set_xticks([0, 1])
axes[1].set_xticklabels(['Fact (0)', 'Myth (1)'])


plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming 'clf' is the trained RandomForestClassifier model
# and 'vectorizer' is the fitted TfidfVectorizer from previous cells.
# If not, you would need to train the model and fit the vectorizer first.

# Get feature importances from the trained model
feature_importances = clf.feature_importances_

# Get feature names from the vectorizer
feature_names = vectorizer.get_feature_names_out()

# Create a DataFrame to store feature names and importances
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importances
})

# Sort the features by importance in descending order
feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)

# Select the top N features (e.g., top 20)
top_n = 20
top_features = feature_importance_df.head(top_n)

# Create a bar plot of the top feature importances
plt.figure(figsize=(10, 8))
sns.barplot(x='importance', y='feature', data=top_features)
plt.title(f'Top {top_n} Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score
import numpy as np # Import numpy for mean calculation

# Assuming 'clf' is the trained RandomForestClassifier model
# and 'X_vec', 'y' are the vectorized features and labels from previous cells.
# If not, you would need to load/train them here.

# Perform 5-fold cross-validation
# Using the model trained on SMOTE data if available, otherwise use the original model
# Check if X_train_smote and y_train_smote exist, if so, use the clf trained on SMOTE data
if 'X_train_smote' in locals() and 'y_train_smote' in locals():
    print("Performing cross-validation on SMOTE-trained model...")
    # Re-train clf on the full SMOTE training data before CV if needed,
    # or use the clf trained in cell 2vcSrVbKfEfc directly if it was trained on SMOTE data.
    # Assuming clf from cell 2vcSrVbKfEfc is the desired model.
    cv_scores = cross_val_score(clf, X_train_smote, y_train_smote, cv=5, scoring='accuracy')
else:
    print("Performing cross-validation on original data model...")
    # Assuming clf from cell L_a6AP0peBYh is the desired model trained on original data.
    # If clf from 2vcSrVbKfEfc is the only trained model, use it here on original data for CV.
    # For consistency with the model trained on SMOTE data, let's use the clf from 2vcSrVbKfEfc if it exists.
    # Otherwise, fall back to clf from L_a6AP0peBYh if it exists.
    if 'clf' in locals():
         cv_scores = cross_val_score(clf, X_vec, y, cv=5, scoring='accuracy')
    else:
         print("Error: No trained model (clf) found. Please run model training cell first.")
         cv_scores = []


if cv_scores is not None and len(cv_scores) > 0:
    print(f"Cross-validation scores: {cv_scores}")
    mean_cv_accuracy = np.mean(cv_scores) # Calculate the mean
    print(f"Mean CV Accuracy: {mean_cv_accuracy:.3f}")

    # Plot cross-validation scores as a line plot
    plt.figure(figsize=(8, 6))
    plt.plot(range(1, len(cv_scores) + 1), cv_scores, marker='o', linestyle='-', color='skyblue', label='Fold Accuracy')
    plt.axhline(mean_cv_accuracy, color='red', linestyle='--', label=f'Mean Accuracy ({mean_cv_accuracy:.3f})') # Add dotted line for mean
    plt.title('Cross-validation Accuracy per Fold')
    plt.xlabel('Fold')
    plt.ylabel('Accuracy')
    plt.ylim(0, 1)  # Accuracy is between 0 and 1
    plt.xticks(range(1, len(cv_scores) + 1)) # Ensure x-axis ticks are integers for each fold
    plt.grid(True)
    plt.legend() # Add a legend to show what the lines represent
    plt.show()
else:
    print("Cross-validation could not be performed.")

import pickle
from google.colab import files

# Assuming 'clf' is your trained model and 'vectorizer' is your fitted vectorizer
# If not, you would need to ensure these objects are available in this cell's scope

# Define filenames for the pickle files
model_filename = 'random_forest_model.pkl'
vectorizer_filename = 'tfidf_vectorizer.pkl'

# Save the model
try:
    with open(model_filename, 'wb') as f:
        pickle.dump(clf, f)
    print(f"Model saved to {model_filename}")
    # Provide download link
    files.download(model_filename)
except NameError:
    print("Error: Model object 'clf' not found. Please ensure the model training cell has been run.")
except Exception as e:
    print(f"Error saving model: {e}")


# Save the vectorizer
try:
    with open(vectorizer_filename, 'wb') as f:
        pickle.dump(vectorizer, f)
    print(f"Vectorizer saved to {vectorizer_filename}")
    # Provide download link
    files.download(vectorizer_filename)
except NameError:
    print("Error: Vectorizer object 'vectorizer' not found. Please ensure the vectorizer fitting cell has been run.")
except Exception as e:
    print(f"Error saving vectorizer: {e}")

from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming y_test and y_pred are available from the model training cell
# Calculate and print the accuracy
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
cm = confusion_matrix(y_test, y_pred)

print(f"Model Accuracy: {accuracy:.3f}")
print(f"Model F1 Score (weighted): {f1:.3f}")
print(f"Model Recall (weighted): {recall:.3f}")

print("\nClassification Report:\n", classification_report(y_test, y_pred))
plt.show()
